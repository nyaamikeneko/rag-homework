{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPI1pj4mFavt"
      },
      "source": [
        "## 扱う質問\n",
        "\n",
        "自然科学に関する一般的な質問を取り扱います。特に質問<SUB>5<SUP>はRAGなしでは答えらないものになっています。\n",
        "\n",
        "- 「ヒトの細胞内でATPを生成する主な経路は何ですか？」\n",
        "-「2008年にノーベル物理学賞を受賞した日本人は誰ですか？」\n",
        "-「エッシャーの作品に見られる特徴的な幾何学構造とは何か？」\n",
        "-「日本における高齢化率は2020年時点で何％だったか？」\n",
        "-「2024年3月に京都大学で開催されたiPS細胞に関する医療AIカンファレンスで話題となった技術は何か？」\n",
        "\n",
        "## 扱うモデル\n",
        "\n",
        "「google/gemma-2-2b-jpn-it」を使用します。このモデルは、リリース時期の関係上、以下の特徴を持ちます。\n",
        "\n",
        "- 「Inference Time Scaling」の概念が広まる前に訓練されており、このトピックに関する知識を持たないと想定される\n",
        "- この特性を活かし、純粋なベースライン評価から各手法の効果を観察する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4EN4GmtStsN"
      },
      "source": [
        "# 演習の方針\n",
        "\n",
        "1. **ベースラインモデル評価**  \n",
        "   素のモデルで回答を生成し、講義内容との整合性の低さを観察します。これにより、特別な学習なしでのモデルの限界を確認します。\n",
        "\n",
        "2. **文字起こしデータの活用**  \n",
        "   講義の文字起こしデータを導入し、モデルが講義内容を参照した回答を生成する傾向を観察します。ただし、Retrieval（情報検索）精度の限界から結果は不安定になる可能性があります。\n",
        "\n",
        "3. **チャンク化の導入**  \n",
        "   文字起こしデータをチャンク（小単位）に分割し、より安定して関連コンテンツを取得できるようにします。この段階では文脈理解にまだ課題があることを確認します。\n",
        "\n",
        "4. **Rerankの適用**  \n",
        "   検索結果のランク付けを導入し、より的確で安定した回答を目指します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bla6WHyQStsO"
      },
      "source": [
        "### 演習環境の準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM50WAI7GXwC",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install google-colab-selenium\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2PStE0uqM03"
      },
      "outputs": [],
      "source": [
        "# 演習用のコンテンツを取得\n",
        "!git clone https://github.com/matsuolab/lecture-ai-engineering.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXo_kFASXlvp"
      },
      "outputs": [],
      "source": [
        "# HuggingFace Login\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ_NUIftXwLc"
      },
      "outputs": [],
      "source": [
        "# CUDAが利用可能ならGPUを、それ以外ならCPUをデバイスとして設定\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eTgV8XBPA90"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tV9mO8oXoaM"
      },
      "outputs": [],
      "source": [
        "# モデル(Gemma2)の読み込み\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"google/gemma-2-2b-jpn-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=bnb_config,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "up2aFzB8WlIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piTdVxTfGcc_"
      },
      "source": [
        "# 1. ベースラインモデル評価\n",
        "**まずはベースモデルがどの程度知識を持っているか確かめる**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_output(query):\n",
        "  messages = [\n",
        "      {\"role\": \"user\", \"content\": query},\n",
        "  ]\n",
        "  input_ids = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      add_generation_prompt=True,\n",
        "      return_tensors=\"pt\"\n",
        "  ).to(model.device)\n",
        "\n",
        "  terminators = [\n",
        "      tokenizer.eos_token_id,\n",
        "      tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "  ]\n",
        "\n",
        "  outputs = model.generate(\n",
        "      input_ids,\n",
        "      max_new_tokens=256,\n",
        "      eos_token_id=terminators,\n",
        "      do_sample=False,\n",
        "      # temperature=0.6, # If do_sample=True\n",
        "      # top_p=0.9,  # If do_sample=True\n",
        "  )\n",
        "\n",
        "  response = outputs[0][input_ids.shape[-1]:]\n",
        "  return tokenizer.decode(response, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "huDNYbXW3lOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"ヒトの細胞内でATPを生成する主な経路は何ですか？\",\n",
        "    \"2008年にノーベル物理学賞を受賞した日本人は誰ですか？\",\n",
        "    \"2024年3月に京都大学で開催されたiPS細胞に関する医療AIカンファレンスで話題となった技術は何か？\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    response = generate_output(question)\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {response}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "tr3b75e3dnHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSCNnRf9pJif"
      },
      "source": [
        "### 結果 (ベースモデル)\n",
        "\n",
        "「google/gemma-2-2b-jpn-it」は「ヒトの細胞内でATPを生成する主な経路は何ですか？」について一般的でない知識を提示しました：\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4R-hiKNGyJd"
      },
      "source": [
        "# 2. 文字起こしデータの活用"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 講義内容をソースとして活用 (RAG導入)\n",
        "\n",
        "モデルの回答の事実性を向上させるためにRetrieval Augmented Generation (RAG)技術を導入します：\n",
        "\n",
        "* **知識ソース**: LLM講座第4講における講師の発言内容\n",
        "* **目的**: モデルに「Inference Time Scaling」に関する正確な知識と文脈を提供し、事実に基づいた回答を促す\n",
        "\n",
        "**初期RAG実装（ベーシックアプローチ）**:\n",
        "* **ドキュメント処理**: 音声認識モデル(speech2text)で書き起こした生テキストをそのまま使用\n",
        "* **分割方法**: 「。」（句点）で区切られた文単位でテキストを分割\n",
        "* **検索手法**: シンプルな類似度ベースの検索でクエリに関連する文を抽出\n",
        "* **制約条件**: モデルの入力トークン制限に収まるよう関連文のみを選択"
      ],
      "metadata": {
        "id": "qcZCmKegHyrl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47GvcceyObAl"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "emb_model = SentenceTransformer(\"infly/inf-retriever-v1-1.5b\", trust_remote_code=True)\n",
        "# In case you want to reduce the maximum length:\n",
        "emb_model.max_seq_length = 4096"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPwggQfUS5yl"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/knowledge.txt\", \"r\") as f:\n",
        "  raw_writedown = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxzKF6L2THIw"
      },
      "outputs": [],
      "source": [
        "# ドキュメントを用意する。\n",
        "documents = [text.strip() for text in raw_writedown.split(\"。\")]\n",
        "print(\"ドキュメントサイズ: \", len(documents))\n",
        "print(\"ドキュメントの例: \\n\", documents[4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nK4cYURzTHIx"
      },
      "outputs": [],
      "source": [
        "# Retrievalの実行\n",
        "question = \"ヒトの細胞内でATPを生成する主な経路は何ですか？\"\n",
        "\n",
        "query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n",
        "document_embeddings = emb_model.encode(documents)\n",
        "\n",
        "# 各ドキュメントの類似度スコア\n",
        "scores = (query_embeddings @ document_embeddings.T) * 100\n",
        "print(scores.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"ヒトの細胞内でATPを生成する主な経路は何ですか？\",\n",
        "    \"2008年にノーベル物理学賞を受賞した日本人は誰ですか？\",\n",
        "    \"2024年3月に京都大学で開催されたiPS細胞に関する医療AIカンファレンスで話題となった技術は何か？\"\n",
        "]\n",
        "\n",
        "topk = 3\n",
        "\n",
        "for q in questions:\n",
        "    print(f\"[質問] {q}\\n\")\n",
        "\n",
        "    # クエリ埋め込み生成\n",
        "    query_embeddings = emb_model.encode([q], prompt_name=\"query\")\n",
        "    document_embeddings = emb_model.encode(documents)\n",
        "\n",
        "    # 類似度スコア計算\n",
        "    scores = (query_embeddings @ document_embeddings.T) * 100\n",
        "\n",
        "    # 上位topk文書を表示\n",
        "    for i, index in enumerate(scores.argsort()[0][::-1][:topk]):\n",
        "        print(f\"取得したドキュメント{i+1}: (Score: {scores[0][index]})\")\n",
        "        print(documents[index], \"\\n\")\n",
        "\n",
        "    # 参考文書をまとめてプロンプト作成\n",
        "    references = \"\\n\".join([\"* \" + documents[i] for i in scores.argsort()[0][::-1][:topk]])\n",
        "    query = f\"[参考資料]\\n{references}\\n\\n[質問] {q}\"\n",
        "\n",
        "    # 回答生成\n",
        "    response = generate_output(query)\n",
        "    print(f\"[回答]\\n{response}\")\n",
        "    print(\"=\" * 80)\n"
      ],
      "metadata": {
        "id": "otACNEJ1eEBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn7tih0RTTzr"
      },
      "source": [
        "### 結果\n",
        "\n",
        "講義内容のドキュメントを追加したにもかかわらず、モデルの回答には依然として以下の問題が見られます：\n",
        "* 「高速に推論する」など、従来の一般的な推論最適化と「Inference Time Scaling」を混同した誤った解釈が継続\n",
        "* 講義内容を参照しているものの、概念の本質を正確に捉えられていない\n",
        "\n",
        "###改善点\n",
        "* aa\n",
        "* bb\n",
        "\n",
        "### 問題分析\n",
        "以下の要因が考えられます：\n",
        "1. **ドキュメント品質の問題**: 音声認識による文字起こしの精度不足\n",
        "2. **検索精度の課題**: 単純な文単位の分割では文脈が失われ、関連性の高いドキュメント片を適切に取得できていない可能性"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 文脈を考慮したチャンク化の導入\n",
        "\n",
        "検索結果の品質向上のため、以下の改善を実施します：\n",
        "\n",
        "* **前後文脈を含むチャンク化**:\n",
        "  - 検索でマッチした文だけでなく、その前後の複数文も含めてチャンクとして取得\n",
        "  - 具体的には、マッチした文を中心に前2文、後2文を含む計5文程度のチャンクを構成\n",
        "  - この「文脈ウィンドウ」により、発言の背景情報や議論の流れが保持される\n",
        "\n",
        "* **期待される効果**:\n",
        "  - 講師の主張とその根拠の関係性を正確に把握できる\n",
        "  - 概念の定義とその適用範囲を正しく理解できる\n",
        "\n",
        "この改善により、モデルが講義内容の本質をより正確に理解し、一貫性のある事実に基づいた回答を生成することが期待されます。"
      ],
      "metadata": {
        "id": "QE3VhpppWejB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94uovDFrVOTJ"
      },
      "outputs": [],
      "source": [
        "# 前後それぞれ2つずつの文章を一つのドキュメントに追加する。（要は5つの文章集合になる)\n",
        "references = \"\\n\".join([\"* \" + \"。\".join(documents[max(0, i-2): min(i+2, len(documents))]).strip() for i in scores.argsort()[0][::-1][:topk]])\n",
        "query =  f\"[参考資料]\\n{references}\\n\\n[質問] ヒトの細胞内でATPを生成する主な経路は何ですか？\"\n",
        "response = generate_output(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"ヒトの細胞内でATPを生成する主な経路は何ですか？\",\n",
        "    \"2008年にノーベル物理学賞を受賞した日本人は誰ですか？\",\n",
        "    \"2024年3月に京都大学で開催されたiPS細胞に関する医療AIカンファレンスで話題となった技術は何か？\"\n",
        "]\n",
        "\n",
        "topk = 3\n",
        "\n",
        "for q in questions:\n",
        "    print(f\"[質問] {q}\\n\")\n",
        "\n",
        "    # クエリの埋め込み\n",
        "    query_embeddings = emb_model.encode([q], prompt_name=\"query\")\n",
        "    document_embeddings = emb_model.encode(documents)\n",
        "\n",
        "    # 類似度スコア計算\n",
        "    scores = (query_embeddings @ document_embeddings.T) * 100\n",
        "\n",
        "    # 前後2文ずつまとめて5文のチャンクを作成\n",
        "    retrieved_chunks = []\n",
        "    for i in scores.argsort()[0][::-1][:topk]:\n",
        "        start = max(0, i - 2)\n",
        "        end = min(i + 3, len(documents))  # +3 because slicing is exclusive\n",
        "        chunk = \"。\".join(documents[start:end]).strip()\n",
        "        retrieved_chunks.append(\"* \" + chunk)\n",
        "\n",
        "    references = \"\\n\".join(retrieved_chunks)\n",
        "\n",
        "    # 質問と参照資料を結合\n",
        "    query = f\"[参考資料]\\n{references}\\n\\n[質問] {q}\"\n",
        "\n",
        "    # 回答生成\n",
        "    response = generate_output(query)\n",
        "    print(f\"[回答]\\n{response}\")\n",
        "    print(\"=\" * 80)\n"
      ],
      "metadata": {
        "id": "qYucR8d2e5ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD3R54G1WX8B"
      },
      "source": [
        "## 結果 (文脈付きチャンク化によるRAG)\n",
        "\n",
        "文脈を含むチャンク化により、モデルの回答の方向性に明確な改善が見られました：\n",
        "\n",
        "### 改善点\n",
        "* 「推論時の計算をスケールさせる」という概念を据えて回答\n",
        "* Inference Time Scalingの基本原理についての理解が向上\n",
        "\n",
        "### 残存する問題点\n",
        "* 質問と関連性の低い情報（ノイズ）が混入する\n",
        "\n",
        "### 問題分析\n",
        "\n",
        "文脈付きチャンク化によるアプローチで新たに発生した課題：\n",
        "\n",
        "1. **情報過多の問題**:\n",
        "   * ドキュメント量の増加により、モデルに提供される情報総量が大幅に増加\n",
        "   * 関連情報と非関連情報が混在し、ノイズと重要情報の区別が困難に\n",
        "\n",
        "2. **情報選択の複雑化**:\n",
        "   * モデルは単に回答を生成するだけでなく、提供された多様な情報源から関連性の高い情報を選別する作業も担うことになった\n",
        "   * この二重タスクにより回答生成の難易度が上昇\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Rerankによる情報品質の向上\n",
        "\n",
        "検索精度をさらに向上させるため、二段階の検索プロセスを導入します：\n",
        "\n",
        "* **Rerank手法の導入**:\n",
        "  - 第一段階: 従来通り基本的な検索アルゴリズムでtop-k個のドキュメントチャンクを取得\n",
        "  - 第二段階: 取得したチャンクに対してLLMを活用した高度な関連性評価を実施\n",
        "  - LLMに「このドキュメントは質問『LLMにおけるInference Time Scalingとは？』に対して本当に関連性が高いか」を判断させる\n",
        "  - 関連性スコアに基づいてランク付けし、真に関連性の高いチャンクのみを選出\n",
        "\n",
        "* **期待される効果**:\n",
        "  - 質の高い情報に焦点を絞ることで、ノイズとなる情報を大幅に削減\n",
        "  - 文脈を保ちながらも、関連性の高い情報のみをモデルに提供\n",
        "  - モデルのタスクを「多量の情報から選別して回答」から「厳選された情報に基づいて回答」へと単純化\n",
        "\n",
        "この高度な情報フィルタリングにより、Inference Time Scalingに関する正確で一貫性のある回答生成が期待されます。"
      ],
      "metadata": {
        "id": "BP0cUKrUZmOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"ヒトの細胞内でATPを生成する主な経路は何ですか？\",\n",
        "    \"2008年にノーベル物理学賞を受賞した日本人は誰ですか？\",\n",
        "    \"2024年3月に京都大学で開催されたiPS細胞に関する医療AIカンファレンスで話題となった技術は何か？\"\n",
        "]\n",
        "\n",
        "topk = 3\n",
        "\n",
        "for question in questions:\n",
        "    print(f\"\\n{'='*80}\\n[質問] {question}\\n\")\n",
        "\n",
        "    # クエリの埋め込みとスコア計算\n",
        "    query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n",
        "    document_embeddings = emb_model.encode(documents)\n",
        "    scores = (query_embeddings @ document_embeddings.T) * 100\n",
        "\n",
        "    # 関連文フィルタリング\n",
        "    references = []\n",
        "    candidate_refs = [\n",
        "        \"。\".join(documents[max(0, i-2): min(i+3, len(documents))]).strip()\n",
        "        for i in scores.argsort()[0][::-1][:topk]\n",
        "    ]\n",
        "\n",
        "    for ref in candidate_refs:\n",
        "        relevance_query = (\n",
        "            f\"与えられた[参考資料]が[質問]に直接関連しているかを、'yes''no'で答えること。\\n\"\n",
        "            f\"[参考資料]\\n{ref}\\n\\n[質問] {question}\"\n",
        "        )\n",
        "        response = generate_output(relevance_query)\n",
        "\n",
        "        print(\"▼ 対象ドキュメント:\\n\", ref.replace(\"。\", \"。\\n\"))\n",
        "        print(\"→ 関連しているか: \", response)\n",
        "\n",
        "        if \"yes\" in response.lower():\n",
        "            references.append(ref)\n",
        "\n",
        "    # 最終プロンプトと回答生成\n",
        "    if references:\n",
        "        joined_refs = \"\\n\".join([\"* \" + r for r in references])\n",
        "        query = f\"[参考資料]\\n{joined_refs}\\n\\n[質問] {question}\"\n",
        "    else:\n",
        "        query = f\"[質問] {question}（参考資料なし）\"\n",
        "\n",
        "    final_response = generate_output(query)\n",
        "    print(f\"\\n[最終回答]\\n{final_response}\")\n"
      ],
      "metadata": {
        "id": "m5p3HtVufgbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elqD2gJt5RCo"
      },
      "source": [
        "## 結果 (Rerank導入後)\n",
        "\n",
        "Rerankの導入により、回答品質に改善が見られました：\n",
        "\n",
        "### 達成された成果\n",
        "* Inference Time Scalingに関する正確な情報を含んだ回答の生成\n",
        "* 無関係な情報やノイズの排除\n",
        "* 講義内容を反映した説明の実現 🎉\n",
        "\n",
        "この結果から、RAGパイプラインにおける情報の質と関連性の重要性であり、検索で取得した情報を単に増やすだけでなく、その情報の関連性を精査する方法を学ぶことができました。"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}