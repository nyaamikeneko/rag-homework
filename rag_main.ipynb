{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPI1pj4mFavt"
      },
      "source": [
        "## æ‰±ã†è³ªå•\n",
        "\n",
        "è‡ªç„¶ç§‘å­¦ã«é–¢ã™ã‚‹ä¸€èˆ¬çš„ãªè³ªå•ã‚’å–ã‚Šæ‰±ã„ã¾ã™ã€‚ç‰¹ã«è³ªå•<SUB>5<SUP>ã¯RAGãªã—ã§ã¯ç­”ãˆã‚‰ãªã„ã‚‚ã®ã«ãªã£ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "- ã€Œãƒ’ãƒˆã®ç´°èƒå†…ã§ATPã‚’ç”Ÿæˆã™ã‚‹ä¸»ãªçµŒè·¯ã¯ä½•ã§ã™ã‹ï¼Ÿã€\n",
        "-ã€Œ2008å¹´ã«ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³ã‚’å—è³ã—ãŸæ—¥æœ¬äººã¯èª°ã§ã™ã‹ï¼Ÿã€\n",
        "-ã€Œã‚¨ãƒƒã‚·ãƒ£ãƒ¼ã®ä½œå“ã«è¦‹ã‚‰ã‚Œã‚‹ç‰¹å¾´çš„ãªå¹¾ä½•å­¦æ§‹é€ ã¨ã¯ä½•ã‹ï¼Ÿã€\n",
        "-ã€Œæ—¥æœ¬ã«ãŠã‘ã‚‹é«˜é½¢åŒ–ç‡ã¯2020å¹´æ™‚ç‚¹ã§ä½•ï¼…ã ã£ãŸã‹ï¼Ÿã€\n",
        "-ã€Œ2024å¹´3æœˆã«äº¬éƒ½å¤§å­¦ã§é–‹å‚¬ã•ã‚ŒãŸiPSç´°èƒã«é–¢ã™ã‚‹åŒ»ç™‚AIã‚«ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã§è©±é¡Œã¨ãªã£ãŸæŠ€è¡“ã¯ä½•ã‹ï¼Ÿã€\n",
        "\n",
        "## æ‰±ã†ãƒ¢ãƒ‡ãƒ«\n",
        "\n",
        "ã€Œgoogle/gemma-2-2b-jpn-itã€ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒªãƒªãƒ¼ã‚¹æ™‚æœŸã®é–¢ä¿‚ä¸Šã€ä»¥ä¸‹ã®ç‰¹å¾´ã‚’æŒã¡ã¾ã™ã€‚\n",
        "\n",
        "- ã€ŒInference Time Scalingã€ã®æ¦‚å¿µãŒåºƒã¾ã‚‹å‰ã«è¨“ç·´ã•ã‚Œã¦ãŠã‚Šã€ã“ã®ãƒˆãƒ”ãƒƒã‚¯ã«é–¢ã™ã‚‹çŸ¥è­˜ã‚’æŒãŸãªã„ã¨æƒ³å®šã•ã‚Œã‚‹\n",
        "- ã“ã®ç‰¹æ€§ã‚’æ´»ã‹ã—ã€ç´”ç²‹ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‹ã‚‰å„æ‰‹æ³•ã®åŠ¹æœã‚’è¦³å¯Ÿã™ã‚‹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4EN4GmtStsN"
      },
      "source": [
        "# æ¼”ç¿’ã®æ–¹é‡\n",
        "\n",
        "1. **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«è©•ä¾¡**  \n",
        "   ç´ ã®ãƒ¢ãƒ‡ãƒ«ã§å›ç­”ã‚’ç”Ÿæˆã—ã€è¬›ç¾©å†…å®¹ã¨ã®æ•´åˆæ€§ã®ä½ã•ã‚’è¦³å¯Ÿã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ç‰¹åˆ¥ãªå­¦ç¿’ãªã—ã§ã®ãƒ¢ãƒ‡ãƒ«ã®é™ç•Œã‚’ç¢ºèªã—ã¾ã™ã€‚\n",
        "\n",
        "2. **æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨**  \n",
        "   è¬›ç¾©ã®æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã‚’å°å…¥ã—ã€ãƒ¢ãƒ‡ãƒ«ãŒè¬›ç¾©å†…å®¹ã‚’å‚ç…§ã—ãŸå›ç­”ã‚’ç”Ÿæˆã™ã‚‹å‚¾å‘ã‚’è¦³å¯Ÿã—ã¾ã™ã€‚ãŸã ã—ã€Retrievalï¼ˆæƒ…å ±æ¤œç´¢ï¼‰ç²¾åº¦ã®é™ç•Œã‹ã‚‰çµæœã¯ä¸å®‰å®šã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "3. **ãƒãƒ£ãƒ³ã‚¯åŒ–ã®å°å…¥**  \n",
        "   æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ£ãƒ³ã‚¯ï¼ˆå°å˜ä½ï¼‰ã«åˆ†å‰²ã—ã€ã‚ˆã‚Šå®‰å®šã—ã¦é–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚ã“ã®æ®µéšã§ã¯æ–‡è„ˆç†è§£ã«ã¾ã èª²é¡ŒãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚\n",
        "\n",
        "4. **Rerankã®é©ç”¨**  \n",
        "   æ¤œç´¢çµæœã®ãƒ©ãƒ³ã‚¯ä»˜ã‘ã‚’å°å…¥ã—ã€ã‚ˆã‚Šçš„ç¢ºã§å®‰å®šã—ãŸå›ç­”ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bla6WHyQStsO"
      },
      "source": [
        "### æ¼”ç¿’ç’°å¢ƒã®æº–å‚™"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM50WAI7GXwC",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install google-colab-selenium\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2PStE0uqM03"
      },
      "outputs": [],
      "source": [
        "# æ¼”ç¿’ç”¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—\n",
        "!git clone https://github.com/matsuolab/lecture-ai-engineering.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXo_kFASXlvp"
      },
      "outputs": [],
      "source": [
        "# HuggingFace Login\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ_NUIftXwLc"
      },
      "outputs": [],
      "source": [
        "# CUDAãŒåˆ©ç”¨å¯èƒ½ãªã‚‰GPUã‚’ã€ãã‚Œä»¥å¤–ãªã‚‰CPUã‚’ãƒ‡ãƒã‚¤ã‚¹ã¨ã—ã¦è¨­å®š\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eTgV8XBPA90"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tV9mO8oXoaM"
      },
      "outputs": [],
      "source": [
        "# ãƒ¢ãƒ‡ãƒ«(Gemma2)ã®èª­ã¿è¾¼ã¿\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"google/gemma-2-2b-jpn-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=bnb_config,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "up2aFzB8WlIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piTdVxTfGcc_"
      },
      "source": [
        "# 1. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«è©•ä¾¡\n",
        "**ã¾ãšã¯ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãŒã©ã®ç¨‹åº¦çŸ¥è­˜ã‚’æŒã£ã¦ã„ã‚‹ã‹ç¢ºã‹ã‚ã‚‹**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_output(query):\n",
        "  messages = [\n",
        "      {\"role\": \"user\", \"content\": query},\n",
        "  ]\n",
        "  input_ids = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      add_generation_prompt=True,\n",
        "      return_tensors=\"pt\"\n",
        "  ).to(model.device)\n",
        "\n",
        "  terminators = [\n",
        "      tokenizer.eos_token_id,\n",
        "      tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "  ]\n",
        "\n",
        "  outputs = model.generate(\n",
        "      input_ids,\n",
        "      max_new_tokens=256,\n",
        "      eos_token_id=terminators,\n",
        "      do_sample=False,\n",
        "      # temperature=0.6, # If do_sample=True\n",
        "      # top_p=0.9,  # If do_sample=True\n",
        "  )\n",
        "\n",
        "  response = outputs[0][input_ids.shape[-1]:]\n",
        "  return tokenizer.decode(response, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "huDNYbXW3lOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"ãƒ’ãƒˆã®ç´°èƒå†…ã§ATPã‚’ç”Ÿæˆã™ã‚‹ä¸»ãªçµŒè·¯ã¯ä½•ã§ã™ã‹ï¼Ÿ\",\n",
        "    \"2008å¹´ã«ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³ã‚’å—è³ã—ãŸæ—¥æœ¬äººã¯èª°ã§ã™ã‹ï¼Ÿ\",\n",
        "    \"2024å¹´3æœˆã«äº¬éƒ½å¤§å­¦ã§é–‹å‚¬ã•ã‚ŒãŸiPSç´°èƒã«é–¢ã™ã‚‹åŒ»ç™‚AIã‚«ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã§è©±é¡Œã¨ãªã£ãŸæŠ€è¡“ã¯ä½•ã‹ï¼Ÿ\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    response = generate_output(question)\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {response}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "tr3b75e3dnHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSCNnRf9pJif"
      },
      "source": [
        "### çµæœ (ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«)\n",
        "\n",
        "ã€Œgoogle/gemma-2-2b-jpn-itã€ã¯ã€Œãƒ’ãƒˆã®ç´°èƒå†…ã§ATPã‚’ç”Ÿæˆã™ã‚‹ä¸»ãªçµŒè·¯ã¯ä½•ã§ã™ã‹ï¼Ÿã€ã«ã¤ã„ã¦ä¸€èˆ¬çš„ã§ãªã„çŸ¥è­˜ã‚’æç¤ºã—ã¾ã—ãŸï¼š\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4R-hiKNGyJd"
      },
      "source": [
        "# 2. æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 è¬›ç¾©å†…å®¹ã‚’ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æ´»ç”¨ (RAGå°å…¥)\n",
        "\n",
        "ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã®äº‹å®Ÿæ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«Retrieval Augmented Generation (RAG)æŠ€è¡“ã‚’å°å…¥ã—ã¾ã™ï¼š\n",
        "\n",
        "* **çŸ¥è­˜ã‚½ãƒ¼ã‚¹**: LLMè¬›åº§ç¬¬4è¬›ã«ãŠã‘ã‚‹è¬›å¸«ã®ç™ºè¨€å†…å®¹\n",
        "* **ç›®çš„**: ãƒ¢ãƒ‡ãƒ«ã«ã€ŒInference Time Scalingã€ã«é–¢ã™ã‚‹æ­£ç¢ºãªçŸ¥è­˜ã¨æ–‡è„ˆã‚’æä¾›ã—ã€äº‹å®Ÿã«åŸºã¥ã„ãŸå›ç­”ã‚’ä¿ƒã™\n",
        "\n",
        "**åˆæœŸRAGå®Ÿè£…ï¼ˆãƒ™ãƒ¼ã‚·ãƒƒã‚¯ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰**:\n",
        "* **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†**: éŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«(speech2text)ã§æ›¸ãèµ·ã“ã—ãŸç”Ÿãƒ†ã‚­ã‚¹ãƒˆã‚’ãã®ã¾ã¾ä½¿ç”¨\n",
        "* **åˆ†å‰²æ–¹æ³•**: ã€Œã€‚ã€ï¼ˆå¥ç‚¹ï¼‰ã§åŒºåˆ‡ã‚‰ã‚ŒãŸæ–‡å˜ä½ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²\n",
        "* **æ¤œç´¢æ‰‹æ³•**: ã‚·ãƒ³ãƒ—ãƒ«ãªé¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢ã§ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹æ–‡ã‚’æŠ½å‡º\n",
        "* **åˆ¶ç´„æ¡ä»¶**: ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã«åã¾ã‚‹ã‚ˆã†é–¢é€£æ–‡ã®ã¿ã‚’é¸æŠ"
      ],
      "metadata": {
        "id": "qcZCmKegHyrl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47GvcceyObAl"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "emb_model = SentenceTransformer(\"infly/inf-retriever-v1-1.5b\", trust_remote_code=True)\n",
        "# In case you want to reduce the maximum length:\n",
        "emb_model.max_seq_length = 4096"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPwggQfUS5yl"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/knowledge.txt\", \"r\") as f:\n",
        "  raw_writedown = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxzKF6L2THIw"
      },
      "outputs": [],
      "source": [
        "# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç”¨æ„ã™ã‚‹ã€‚\n",
        "documents = [text.strip() for text in raw_writedown.split(\"ã€‚\")]\n",
        "print(\"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚µã‚¤ã‚º: \", len(documents))\n",
        "print(\"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä¾‹: \\n\", documents[4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nK4cYURzTHIx"
      },
      "outputs": [],
      "source": [
        "# Retrievalã®å®Ÿè¡Œ\n",
        "question = \"ãƒ’ãƒˆã®ç´°èƒå†…ã§ATPã‚’ç”Ÿæˆã™ã‚‹ä¸»ãªçµŒè·¯ã¯ä½•ã§ã™ã‹ï¼Ÿ\"\n",
        "\n",
        "query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n",
        "document_embeddings = emb_model.encode(documents)\n",
        "\n",
        "# å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢\n",
        "scores = (query_embeddings @ document_embeddings.T) * 100\n",
        "print(scores.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"ãƒ’ãƒˆã®ç´°èƒå†…ã§ATPã‚’ç”Ÿæˆã™ã‚‹ä¸»ãªçµŒè·¯ã¯ä½•ã§ã™ã‹ï¼Ÿ\",\n",
        "    \"2008å¹´ã«ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³ã‚’å—è³ã—ãŸæ—¥æœ¬äººã¯èª°ã§ã™ã‹ï¼Ÿ\",\n",
        "    \"2024å¹´3æœˆã«äº¬éƒ½å¤§å­¦ã§é–‹å‚¬ã•ã‚ŒãŸiPSç´°èƒã«é–¢ã™ã‚‹åŒ»ç™‚AIã‚«ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã§è©±é¡Œã¨ãªã£ãŸæŠ€è¡“ã¯ä½•ã‹ï¼Ÿ\"\n",
        "]\n",
        "\n",
        "topk = 3\n",
        "\n",
        "for q in questions:\n",
        "    print(f\"[è³ªå•] {q}\\n\")\n",
        "\n",
        "    # ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ\n",
        "    query_embeddings = emb_model.encode([q], prompt_name=\"query\")\n",
        "    document_embeddings = emb_model.encode(documents)\n",
        "\n",
        "    # é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—\n",
        "    scores = (query_embeddings @ document_embeddings.T) * 100\n",
        "\n",
        "    # ä¸Šä½topkæ–‡æ›¸ã‚’è¡¨ç¤º\n",
        "    for i, index in enumerate(scores.argsort()[0][::-1][:topk]):\n",
        "        print(f\"å–å¾—ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ{i+1}: (Score: {scores[0][index]})\")\n",
        "        print(documents[index], \"\\n\")\n",
        "\n",
        "    # å‚è€ƒæ–‡æ›¸ã‚’ã¾ã¨ã‚ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ\n",
        "    references = \"\\n\".join([\"* \" + documents[i] for i in scores.argsort()[0][::-1][:topk]])\n",
        "    query = f\"[å‚è€ƒè³‡æ–™]\\n{references}\\n\\n[è³ªå•] {q}\"\n",
        "\n",
        "    # å›ç­”ç”Ÿæˆ\n",
        "    response = generate_output(query)\n",
        "    print(f\"[å›ç­”]\\n{response}\")\n",
        "    print(\"=\" * 80)\n"
      ],
      "metadata": {
        "id": "otACNEJ1eEBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn7tih0RTTzr"
      },
      "source": [
        "### çµæœ\n",
        "\n",
        "è¬›ç¾©å†…å®¹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ ã—ãŸã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã«ã¯ä¾ç„¶ã¨ã—ã¦ä»¥ä¸‹ã®å•é¡ŒãŒè¦‹ã‚‰ã‚Œã¾ã™ï¼š\n",
        "* ã€Œé«˜é€Ÿã«æ¨è«–ã™ã‚‹ã€ãªã©ã€å¾“æ¥ã®ä¸€èˆ¬çš„ãªæ¨è«–æœ€é©åŒ–ã¨ã€ŒInference Time Scalingã€ã‚’æ··åŒã—ãŸèª¤ã£ãŸè§£é‡ˆãŒç¶™ç¶š\n",
        "* è¬›ç¾©å†…å®¹ã‚’å‚ç…§ã—ã¦ã„ã‚‹ã‚‚ã®ã®ã€æ¦‚å¿µã®æœ¬è³ªã‚’æ­£ç¢ºã«æ‰ãˆã‚‰ã‚Œã¦ã„ãªã„\n",
        "\n",
        "###æ”¹å–„ç‚¹\n",
        "* aa\n",
        "* bb\n",
        "\n",
        "### å•é¡Œåˆ†æ\n",
        "ä»¥ä¸‹ã®è¦å› ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ï¼š\n",
        "1. **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå“è³ªã®å•é¡Œ**: éŸ³å£°èªè­˜ã«ã‚ˆã‚‹æ–‡å­—èµ·ã“ã—ã®ç²¾åº¦ä¸è¶³\n",
        "2. **æ¤œç´¢ç²¾åº¦ã®èª²é¡Œ**: å˜ç´”ãªæ–‡å˜ä½ã®åˆ†å‰²ã§ã¯æ–‡è„ˆãŒå¤±ã‚ã‚Œã€é–¢é€£æ€§ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç‰‡ã‚’é©åˆ‡ã«å–å¾—ã§ãã¦ã„ãªã„å¯èƒ½æ€§"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸãƒãƒ£ãƒ³ã‚¯åŒ–ã®å°å…¥\n",
        "\n",
        "æ¤œç´¢çµæœã®å“è³ªå‘ä¸Šã®ãŸã‚ã€ä»¥ä¸‹ã®æ”¹å–„ã‚’å®Ÿæ–½ã—ã¾ã™ï¼š\n",
        "\n",
        "* **å‰å¾Œæ–‡è„ˆã‚’å«ã‚€ãƒãƒ£ãƒ³ã‚¯åŒ–**:\n",
        "  - æ¤œç´¢ã§ãƒãƒƒãƒã—ãŸæ–‡ã ã‘ã§ãªãã€ãã®å‰å¾Œã®è¤‡æ•°æ–‡ã‚‚å«ã‚ã¦ãƒãƒ£ãƒ³ã‚¯ã¨ã—ã¦å–å¾—\n",
        "  - å…·ä½“çš„ã«ã¯ã€ãƒãƒƒãƒã—ãŸæ–‡ã‚’ä¸­å¿ƒã«å‰2æ–‡ã€å¾Œ2æ–‡ã‚’å«ã‚€è¨ˆ5æ–‡ç¨‹åº¦ã®ãƒãƒ£ãƒ³ã‚¯ã‚’æ§‹æˆ\n",
        "  - ã“ã®ã€Œæ–‡è„ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã€ã«ã‚ˆã‚Šã€ç™ºè¨€ã®èƒŒæ™¯æƒ…å ±ã‚„è­°è«–ã®æµã‚ŒãŒä¿æŒã•ã‚Œã‚‹\n",
        "\n",
        "* **æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ**:\n",
        "  - è¬›å¸«ã®ä¸»å¼µã¨ãã®æ ¹æ‹ ã®é–¢ä¿‚æ€§ã‚’æ­£ç¢ºã«æŠŠæ¡ã§ãã‚‹\n",
        "  - æ¦‚å¿µã®å®šç¾©ã¨ãã®é©ç”¨ç¯„å›²ã‚’æ­£ã—ãç†è§£ã§ãã‚‹\n",
        "\n",
        "ã“ã®æ”¹å–„ã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ãŒè¬›ç¾©å†…å®¹ã®æœ¬è³ªã‚’ã‚ˆã‚Šæ­£ç¢ºã«ç†è§£ã—ã€ä¸€è²«æ€§ã®ã‚ã‚‹äº‹å®Ÿã«åŸºã¥ã„ãŸå›ç­”ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "QE3VhpppWejB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94uovDFrVOTJ"
      },
      "outputs": [],
      "source": [
        "# å‰å¾Œãã‚Œãã‚Œ2ã¤ãšã¤ã®æ–‡ç« ã‚’ä¸€ã¤ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«è¿½åŠ ã™ã‚‹ã€‚ï¼ˆè¦ã¯5ã¤ã®æ–‡ç« é›†åˆã«ãªã‚‹)\n",
        "references = \"\\n\".join([\"* \" + \"ã€‚\".join(documents[max(0, i-2): min(i+2, len(documents))]).strip() for i in scores.argsort()[0][::-1][:topk]])\n",
        "query =  f\"[å‚è€ƒè³‡æ–™]\\n{references}\\n\\n[è³ªå•] ãƒ’ãƒˆã®ç´°èƒå†…ã§ATPã‚’ç”Ÿæˆã™ã‚‹ä¸»ãªçµŒè·¯ã¯ä½•ã§ã™ã‹ï¼Ÿ\"\n",
        "response = generate_output(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"ãƒ’ãƒˆã®ç´°èƒå†…ã§ATPã‚’ç”Ÿæˆã™ã‚‹ä¸»ãªçµŒè·¯ã¯ä½•ã§ã™ã‹ï¼Ÿ\",\n",
        "    \"2008å¹´ã«ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³ã‚’å—è³ã—ãŸæ—¥æœ¬äººã¯èª°ã§ã™ã‹ï¼Ÿ\",\n",
        "    \"2024å¹´3æœˆã«äº¬éƒ½å¤§å­¦ã§é–‹å‚¬ã•ã‚ŒãŸiPSç´°èƒã«é–¢ã™ã‚‹åŒ»ç™‚AIã‚«ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã§è©±é¡Œã¨ãªã£ãŸæŠ€è¡“ã¯ä½•ã‹ï¼Ÿ\"\n",
        "]\n",
        "\n",
        "topk = 3\n",
        "\n",
        "for q in questions:\n",
        "    print(f\"[è³ªå•] {q}\\n\")\n",
        "\n",
        "    # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿\n",
        "    query_embeddings = emb_model.encode([q], prompt_name=\"query\")\n",
        "    document_embeddings = emb_model.encode(documents)\n",
        "\n",
        "    # é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—\n",
        "    scores = (query_embeddings @ document_embeddings.T) * 100\n",
        "\n",
        "    # å‰å¾Œ2æ–‡ãšã¤ã¾ã¨ã‚ã¦5æ–‡ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆ\n",
        "    retrieved_chunks = []\n",
        "    for i in scores.argsort()[0][::-1][:topk]:\n",
        "        start = max(0, i - 2)\n",
        "        end = min(i + 3, len(documents))  # +3 because slicing is exclusive\n",
        "        chunk = \"ã€‚\".join(documents[start:end]).strip()\n",
        "        retrieved_chunks.append(\"* \" + chunk)\n",
        "\n",
        "    references = \"\\n\".join(retrieved_chunks)\n",
        "\n",
        "    # è³ªå•ã¨å‚ç…§è³‡æ–™ã‚’çµåˆ\n",
        "    query = f\"[å‚è€ƒè³‡æ–™]\\n{references}\\n\\n[è³ªå•] {q}\"\n",
        "\n",
        "    # å›ç­”ç”Ÿæˆ\n",
        "    response = generate_output(query)\n",
        "    print(f\"[å›ç­”]\\n{response}\")\n",
        "    print(\"=\" * 80)\n"
      ],
      "metadata": {
        "id": "qYucR8d2e5ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD3R54G1WX8B"
      },
      "source": [
        "## çµæœ (æ–‡è„ˆä»˜ããƒãƒ£ãƒ³ã‚¯åŒ–ã«ã‚ˆã‚‹RAG)\n",
        "\n",
        "æ–‡è„ˆã‚’å«ã‚€ãƒãƒ£ãƒ³ã‚¯åŒ–ã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã®æ–¹å‘æ€§ã«æ˜ç¢ºãªæ”¹å–„ãŒè¦‹ã‚‰ã‚Œã¾ã—ãŸï¼š\n",
        "\n",
        "### æ”¹å–„ç‚¹\n",
        "* ã€Œæ¨è«–æ™‚ã®è¨ˆç®—ã‚’ã‚¹ã‚±ãƒ¼ãƒ«ã•ã›ã‚‹ã€ã¨ã„ã†æ¦‚å¿µã‚’æ®ãˆã¦å›ç­”\n",
        "* Inference Time Scalingã®åŸºæœ¬åŸç†ã«ã¤ã„ã¦ã®ç†è§£ãŒå‘ä¸Š\n",
        "\n",
        "### æ®‹å­˜ã™ã‚‹å•é¡Œç‚¹\n",
        "* è³ªå•ã¨é–¢é€£æ€§ã®ä½ã„æƒ…å ±ï¼ˆãƒã‚¤ã‚ºï¼‰ãŒæ··å…¥ã™ã‚‹\n",
        "\n",
        "### å•é¡Œåˆ†æ\n",
        "\n",
        "æ–‡è„ˆä»˜ããƒãƒ£ãƒ³ã‚¯åŒ–ã«ã‚ˆã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§æ–°ãŸã«ç™ºç”Ÿã—ãŸèª²é¡Œï¼š\n",
        "\n",
        "1. **æƒ…å ±éå¤šã®å•é¡Œ**:\n",
        "   * ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé‡ã®å¢—åŠ ã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã«æä¾›ã•ã‚Œã‚‹æƒ…å ±ç·é‡ãŒå¤§å¹…ã«å¢—åŠ \n",
        "   * é–¢é€£æƒ…å ±ã¨éé–¢é€£æƒ…å ±ãŒæ··åœ¨ã—ã€ãƒã‚¤ã‚ºã¨é‡è¦æƒ…å ±ã®åŒºåˆ¥ãŒå›°é›£ã«\n",
        "\n",
        "2. **æƒ…å ±é¸æŠã®è¤‡é›‘åŒ–**:\n",
        "   * ãƒ¢ãƒ‡ãƒ«ã¯å˜ã«å›ç­”ã‚’ç”Ÿæˆã™ã‚‹ã ã‘ã§ãªãã€æä¾›ã•ã‚ŒãŸå¤šæ§˜ãªæƒ…å ±æºã‹ã‚‰é–¢é€£æ€§ã®é«˜ã„æƒ…å ±ã‚’é¸åˆ¥ã™ã‚‹ä½œæ¥­ã‚‚æ‹…ã†ã“ã¨ã«ãªã£ãŸ\n",
        "   * ã“ã®äºŒé‡ã‚¿ã‚¹ã‚¯ã«ã‚ˆã‚Šå›ç­”ç”Ÿæˆã®é›£æ˜“åº¦ãŒä¸Šæ˜‡\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Rerankã«ã‚ˆã‚‹æƒ…å ±å“è³ªã®å‘ä¸Š\n",
        "\n",
        "æ¤œç´¢ç²¾åº¦ã‚’ã•ã‚‰ã«å‘ä¸Šã•ã›ã‚‹ãŸã‚ã€äºŒæ®µéšã®æ¤œç´¢ãƒ—ãƒ­ã‚»ã‚¹ã‚’å°å…¥ã—ã¾ã™ï¼š\n",
        "\n",
        "* **Rerankæ‰‹æ³•ã®å°å…¥**:\n",
        "  - ç¬¬ä¸€æ®µéš: å¾“æ¥é€šã‚ŠåŸºæœ¬çš„ãªæ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§top-kå€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—\n",
        "  - ç¬¬äºŒæ®µéš: å–å¾—ã—ãŸãƒãƒ£ãƒ³ã‚¯ã«å¯¾ã—ã¦LLMã‚’æ´»ç”¨ã—ãŸé«˜åº¦ãªé–¢é€£æ€§è©•ä¾¡ã‚’å®Ÿæ–½\n",
        "  - LLMã«ã€Œã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯è³ªå•ã€LLMã«ãŠã‘ã‚‹Inference Time Scalingã¨ã¯ï¼Ÿã€ã«å¯¾ã—ã¦æœ¬å½“ã«é–¢é€£æ€§ãŒé«˜ã„ã‹ã€ã‚’åˆ¤æ–­ã•ã›ã‚‹\n",
        "  - é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã«åŸºã¥ã„ã¦ãƒ©ãƒ³ã‚¯ä»˜ã‘ã—ã€çœŸã«é–¢é€£æ€§ã®é«˜ã„ãƒãƒ£ãƒ³ã‚¯ã®ã¿ã‚’é¸å‡º\n",
        "\n",
        "* **æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ**:\n",
        "  - è³ªã®é«˜ã„æƒ…å ±ã«ç„¦ç‚¹ã‚’çµã‚‹ã“ã¨ã§ã€ãƒã‚¤ã‚ºã¨ãªã‚‹æƒ…å ±ã‚’å¤§å¹…ã«å‰Šæ¸›\n",
        "  - æ–‡è„ˆã‚’ä¿ã¡ãªãŒã‚‰ã‚‚ã€é–¢é€£æ€§ã®é«˜ã„æƒ…å ±ã®ã¿ã‚’ãƒ¢ãƒ‡ãƒ«ã«æä¾›\n",
        "  - ãƒ¢ãƒ‡ãƒ«ã®ã‚¿ã‚¹ã‚¯ã‚’ã€Œå¤šé‡ã®æƒ…å ±ã‹ã‚‰é¸åˆ¥ã—ã¦å›ç­”ã€ã‹ã‚‰ã€Œå³é¸ã•ã‚ŒãŸæƒ…å ±ã«åŸºã¥ã„ã¦å›ç­”ã€ã¸ã¨å˜ç´”åŒ–\n",
        "\n",
        "ã“ã®é«˜åº¦ãªæƒ…å ±ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚Šã€Inference Time Scalingã«é–¢ã™ã‚‹æ­£ç¢ºã§ä¸€è²«æ€§ã®ã‚ã‚‹å›ç­”ç”ŸæˆãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "BP0cUKrUZmOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"ãƒ’ãƒˆã®ç´°èƒå†…ã§ATPã‚’ç”Ÿæˆã™ã‚‹ä¸»ãªçµŒè·¯ã¯ä½•ã§ã™ã‹ï¼Ÿ\",\n",
        "    \"2008å¹´ã«ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³ã‚’å—è³ã—ãŸæ—¥æœ¬äººã¯èª°ã§ã™ã‹ï¼Ÿ\",\n",
        "    \"2024å¹´3æœˆã«äº¬éƒ½å¤§å­¦ã§é–‹å‚¬ã•ã‚ŒãŸiPSç´°èƒã«é–¢ã™ã‚‹åŒ»ç™‚AIã‚«ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã§è©±é¡Œã¨ãªã£ãŸæŠ€è¡“ã¯ä½•ã‹ï¼Ÿ\"\n",
        "]\n",
        "\n",
        "topk = 3\n",
        "\n",
        "for question in questions:\n",
        "    print(f\"\\n{'='*80}\\n[è³ªå•] {question}\\n\")\n",
        "\n",
        "    # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ã¨ã‚¹ã‚³ã‚¢è¨ˆç®—\n",
        "    query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n",
        "    document_embeddings = emb_model.encode(documents)\n",
        "    scores = (query_embeddings @ document_embeddings.T) * 100\n",
        "\n",
        "    # é–¢é€£æ–‡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
        "    references = []\n",
        "    candidate_refs = [\n",
        "        \"ã€‚\".join(documents[max(0, i-2): min(i+3, len(documents))]).strip()\n",
        "        for i in scores.argsort()[0][::-1][:topk]\n",
        "    ]\n",
        "\n",
        "    for ref in candidate_refs:\n",
        "        relevance_query = (\n",
        "            f\"ä¸ãˆã‚‰ã‚ŒãŸ[å‚è€ƒè³‡æ–™]ãŒ[è³ªå•]ã«ç›´æ¥é–¢é€£ã—ã¦ã„ã‚‹ã‹ã‚’ã€'yes''no'ã§ç­”ãˆã‚‹ã“ã¨ã€‚\\n\"\n",
        "            f\"[å‚è€ƒè³‡æ–™]\\n{ref}\\n\\n[è³ªå•] {question}\"\n",
        "        )\n",
        "        response = generate_output(relevance_query)\n",
        "\n",
        "        print(\"â–¼ å¯¾è±¡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:\\n\", ref.replace(\"ã€‚\", \"ã€‚\\n\"))\n",
        "        print(\"â†’ é–¢é€£ã—ã¦ã„ã‚‹ã‹: \", response)\n",
        "\n",
        "        if \"yes\" in response.lower():\n",
        "            references.append(ref)\n",
        "\n",
        "    # æœ€çµ‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å›ç­”ç”Ÿæˆ\n",
        "    if references:\n",
        "        joined_refs = \"\\n\".join([\"* \" + r for r in references])\n",
        "        query = f\"[å‚è€ƒè³‡æ–™]\\n{joined_refs}\\n\\n[è³ªå•] {question}\"\n",
        "    else:\n",
        "        query = f\"[è³ªå•] {question}ï¼ˆå‚è€ƒè³‡æ–™ãªã—ï¼‰\"\n",
        "\n",
        "    final_response = generate_output(query)\n",
        "    print(f\"\\n[æœ€çµ‚å›ç­”]\\n{final_response}\")\n"
      ],
      "metadata": {
        "id": "m5p3HtVufgbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elqD2gJt5RCo"
      },
      "source": [
        "## çµæœ (Rerankå°å…¥å¾Œ)\n",
        "\n",
        "Rerankã®å°å…¥ã«ã‚ˆã‚Šã€å›ç­”å“è³ªã«æ”¹å–„ãŒè¦‹ã‚‰ã‚Œã¾ã—ãŸï¼š\n",
        "\n",
        "### é”æˆã•ã‚ŒãŸæˆæœ\n",
        "* Inference Time Scalingã«é–¢ã™ã‚‹æ­£ç¢ºãªæƒ…å ±ã‚’å«ã‚“ã å›ç­”ã®ç”Ÿæˆ\n",
        "* ç„¡é–¢ä¿‚ãªæƒ…å ±ã‚„ãƒã‚¤ã‚ºã®æ’é™¤\n",
        "* è¬›ç¾©å†…å®¹ã‚’åæ˜ ã—ãŸèª¬æ˜ã®å®Ÿç¾ ğŸ‰\n",
        "\n",
        "ã“ã®çµæœã‹ã‚‰ã€RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ãŠã‘ã‚‹æƒ…å ±ã®è³ªã¨é–¢é€£æ€§ã®é‡è¦æ€§ã§ã‚ã‚Šã€æ¤œç´¢ã§å–å¾—ã—ãŸæƒ…å ±ã‚’å˜ã«å¢—ã‚„ã™ã ã‘ã§ãªãã€ãã®æƒ…å ±ã®é–¢é€£æ€§ã‚’ç²¾æŸ»ã™ã‚‹æ–¹æ³•ã‚’å­¦ã¶ã“ã¨ãŒã§ãã¾ã—ãŸã€‚"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}